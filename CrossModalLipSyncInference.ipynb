{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3ff0b1",
   "metadata": {},
   "source": [
    "# Cross-Modal Lip-Sync Detection: Model Inference Guide\n",
    "\n",
    "This notebook implements the complete inference pipeline for the cross-modal phoneme–viseme alignment model.\n",
    "\n",
    "**Key Concept**: Detect lip-sync manipulation (deepfakes) by comparing audio and visual modalities over time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and adapt to your environment)\n",
    "\"\"\"\n",
    "# Load model\n",
    "model_path = Path(\"task3_alignment_model.pt\")\n",
    "model = load_model_from_checkpoint(model_path)\n",
    "\n",
    "# Analyze a single window\n",
    "video_path = Path(\"sample_video.mp4\")\n",
    "prob = predict_with_model(model, video_path, start_s=0.0, end_s=1.0)\n",
    "print(f\"Window [0.0–1.0s] probability: {prob:.4f}\")\n",
    "\n",
    "# Analyze entire video\n",
    "scores, metadata = predict_video_windows(\n",
    "    model,\n",
    "    video_path,\n",
    "    window_size=1.0,\n",
    "    stride=0.5\n",
    ")\n",
    "print(f\"Overall decision: {'FAKE' if metadata['mean_score'] >= 0.5 else 'REAL'}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "✓ Cross-modal lip-sync inference pipeline ready!\n",
    "\n",
    "To use in your MVP:\n",
    "1. Load model: model = load_model_from_checkpoint(checkpoint_path)\n",
    "2. Single window: prob = predict_with_model(model, video_path, start_s, end_s)\n",
    "3. Full video: scores, meta = predict_video_windows(model, video_path)\n",
    "\n",
    "Outputs:\n",
    "- Probability ≥ 0.5 → FAKE (lip-sync manipulated)\n",
    "- Probability < 0.5 → REAL (authentic)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a57ea",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below is a complete example showing how to use the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path: Path) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load model from saved checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to model weights (.pt file)\n",
    "    \n",
    "    Returns:\n",
    "        Model in eval mode on correct device\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model from {checkpoint_path}...\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        \n",
    "        # Instantiate model\n",
    "        model = CrossModalPhonemeVisemeAlignmentModel()\n",
    "        \n",
    "        # Load weights\n",
    "        if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        logger.info(f\"✓ Model loaded successfully on {DEVICE}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def predict_video_windows(\n",
    "    model: nn.Module,\n",
    "    video_path: Path,\n",
    "    window_size: float = 1.0,\n",
    "    stride: float = 0.5,\n",
    ") -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Run sliding window inference on entire video.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded model\n",
    "        video_path: Path to video file\n",
    "        window_size: Window duration in seconds (default 1.0s)\n",
    "        stride: Stride between windows in seconds (default 0.5s)\n",
    "    \n",
    "    Returns:\n",
    "        - scores: numpy array of probabilities per window\n",
    "        - metadata: dict with window times and statistics\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Get video duration\n",
    "    probe_cmd = f\"ffprobe -v error -show_entries format=duration -of csv=p=0 '{video_path}'\"\n",
    "    result = subprocess.run(probe_cmd, shell=True, capture_output=True, text=True)\n",
    "    duration = float(result.stdout.strip())\n",
    "    \n",
    "    logger.info(f\"Analyzing video: {video_path.name} (duration: {duration:.1f}s)\")\n",
    "    \n",
    "    scores = []\n",
    "    windows = []\n",
    "    \n",
    "    start_s = 0.0\n",
    "    while start_s < duration:\n",
    "        end_s = min(start_s + window_size, duration)\n",
    "        \n",
    "        try:\n",
    "            prob = predict_with_model(model, video_path, start_s, end_s)\n",
    "            scores.append(prob)\n",
    "            windows.append((start_s, end_s))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping window [{start_s:.2f}–{end_s:.2f}s]: {e}\")\n",
    "            scores.append(np.nan)\n",
    "            windows.append((start_s, end_s))\n",
    "        \n",
    "        start_s += stride\n",
    "    \n",
    "    scores_array = np.array(scores)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    valid_scores = scores_array[~np.isnan(scores_array)]\n",
    "    metadata = {\n",
    "        \"duration\": duration,\n",
    "        \"windows\": windows,\n",
    "        \"mean_score\": float(np.mean(valid_scores)),\n",
    "        \"std_score\": float(np.std(valid_scores)),\n",
    "        \"max_score\": float(np.max(valid_scores)),\n",
    "        \"min_score\": float(np.min(valid_scores)),\n",
    "    }\n",
    "    \n",
    "    # Final decision\n",
    "    final_prob = metadata[\"mean_score\"]\n",
    "    if final_prob >= 0.5:\n",
    "        decision = \"FAKE (manipulated)\"\n",
    "    else:\n",
    "        decision = \"REAL (authentic)\"\n",
    "    \n",
    "    logger.info(f\"✓ Analysis complete.\")\n",
    "    logger.info(f\"  Mean probability: {final_prob:.4f}\")\n",
    "    logger.info(f\"  Decision: {decision}\")\n",
    "    \n",
    "    return scores_array, metadata\n",
    "\n",
    "\n",
    "print(\"✓ Full pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabfef6",
   "metadata": {},
   "source": [
    "## Section 7: End-to-End Prediction Pipeline\n",
    "\n",
    "Complete workflow for a full video analysis:\n",
    "1. Load model from checkpoint\n",
    "2. Split video into sliding windows\n",
    "3. Run inference on each window\n",
    "4. Aggregate scores and make final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(\n",
    "    model: nn.Module,\n",
    "    video_path: Path,\n",
    "    start_s: float,\n",
    "    end_s: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Complete inference pipeline for lip-sync manipulation detection.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded CrossModalPhonemeVisemeAlignmentModel\n",
    "        video_path: Path to video file\n",
    "        start_s: Window start time in seconds\n",
    "        end_s: Window end time in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Probability of lip-sync manipulation [0.0, 1.0]\n",
    "        - p ≥ 0.5: likely FAKE (manipulated)\n",
    "        - p < 0.5: likely REAL (authentic)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Predicting window [{start_s:.2f}–{end_s:.2f}s] from {video_path.name}\")\n",
    "        \n",
    "        # Step 1: Extract audio segment\n",
    "        logger.debug(\"  → Extracting audio...\")\n",
    "        audio_waveform = extract_audio_segment(video_path, start_s, end_s)\n",
    "        \n",
    "        # Step 2: Extract video frames (mouth ROI)\n",
    "        logger.debug(\"  → Extracting mouth frames...\")\n",
    "        mouth_frames = extract_mouth_frames(video_path, start_s, end_s)\n",
    "        \n",
    "        # Step 3: Verify temporal alignment\n",
    "        verify_temporal_alignment(audio_waveform, mouth_frames, start_s, end_s)\n",
    "        \n",
    "        # Step 4: Preprocess\n",
    "        logger.debug(\"  → Preprocessing audio...\")\n",
    "        audio_waveform = preprocess_audio(audio_waveform)\n",
    "        \n",
    "        logger.debug(\"  → Preprocessing video...\")\n",
    "        mouth_frames = preprocess_video(mouth_frames)\n",
    "        \n",
    "        # Step 5: Model inference\n",
    "        logger.debug(\"  → Running model forward pass...\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(audio_waveform, mouth_frames)\n",
    "        \n",
    "        # Step 6: Convert logits to probability via sigmoid\n",
    "        probability = torch.sigmoid(logits).squeeze().item()\n",
    "        probability = float(np.clip(probability, 0.0, 1.0))\n",
    "        \n",
    "        logger.info(f\"✓ Inference complete. Probability: {probability:.4f}\")\n",
    "        return probability\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model inference failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"✓ Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8d1a1",
   "metadata": {},
   "source": [
    "## Section 6: Model Inference Function\n",
    "\n",
    "The core inference pipeline: extract → preprocess → forward → sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a13d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_temporal_alignment(\n",
    "    audio_waveform: Tensor,\n",
    "    video_frames: Tensor,\n",
    "    start_s: float,\n",
    "    end_s: float,\n",
    "    audio_sr: int = 16000,\n",
    "    video_fps: int = 25,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Verify that audio and video segments are properly aligned.\n",
    "    \n",
    "    Args:\n",
    "        audio_waveform: Audio tensor [1, num_samples]\n",
    "        video_frames: Video tensor [num_frames, 3, H, W]\n",
    "        start_s: Window start time\n",
    "        end_s: Window end time\n",
    "        audio_sr: Audio sample rate (16 kHz)\n",
    "        video_fps: Video frame rate (25 fps)\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError if temporal alignment is invalid\n",
    "    \"\"\"\n",
    "    window_duration = end_s - start_s\n",
    "    \n",
    "    # Expected audio samples\n",
    "    expected_audio_samples = int(window_duration * audio_sr)\n",
    "    actual_audio_samples = audio_waveform.shape[-1]\n",
    "    \n",
    "    # Expected video frames\n",
    "    expected_frames = int(window_duration * video_fps)\n",
    "    actual_frames = video_frames.shape[0]\n",
    "    \n",
    "    logger.info(\n",
    "        f\"Temporal alignment check for window [{start_s:.2f}–{end_s:.2f}s]:\"\n",
    "    )\n",
    "    logger.info(f\"  Audio: {actual_audio_samples} samples (expected ~{expected_audio_samples})\")\n",
    "    logger.info(f\"  Video: {actual_frames} frames (expected ~{expected_frames})\")\n",
    "    \n",
    "    # Allow 10% tolerance\n",
    "    assert abs(actual_audio_samples - expected_audio_samples) < 0.1 * expected_audio_samples, \\\n",
    "        \"Audio samples out of expected range\"\n",
    "    assert abs(actual_frames - expected_frames) < 0.1 * expected_frames, \\\n",
    "        \"Video frames out of expected range\"\n",
    "    \n",
    "    logger.info(\"✓ Temporal alignment verified\")\n",
    "\n",
    "\n",
    "print(\"✓ Temporal alignment verification defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3067d36",
   "metadata": {},
   "source": [
    "## Section 5: Temporal Alignment and Synchronization\n",
    "\n",
    "Critical: Audio and video segments must be temporally synchronized.\n",
    "Both modalities use the same time window [start_s, end_s] for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "MOUTH_ROI_SIZE = (112, 112)\n",
    "\n",
    "\n",
    "def extract_mouth_roi_simple(frame: np.ndarray) -> Tensor:\n",
    "    \"\"\"\n",
    "    Simple mouth ROI extraction using center crop.\n",
    "    \n",
    "    NOTE: In production, use MediaPipe or dlib for robust face/mouth detection.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame in BGR format [H, W, 3]\n",
    "    \n",
    "    Returns:\n",
    "        Mouth ROI tensor [3, 112, 112]\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Center crop approximation (30% of frame height/width)\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    roi_h, roi_w = int(h * 0.3), int(w * 0.3)\n",
    "    \n",
    "    y_start = max(0, center_y - roi_h // 2)\n",
    "    y_end = min(h, center_y + roi_h // 2)\n",
    "    x_start = max(0, center_x - roi_w // 2)\n",
    "    x_end = min(w, center_x + roi_w // 2)\n",
    "    \n",
    "    mouth_roi = frame[y_start:y_end, x_start:x_end, :]\n",
    "    \n",
    "    # Convert BGR → RGB\n",
    "    mouth_roi = cv2.cvtColor(mouth_roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize to model input size\n",
    "    mouth_roi = cv2.resize(mouth_roi, MOUTH_ROI_SIZE, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Convert to tensor [3, H, W]\n",
    "    mouth_roi_tensor = torch.from_numpy(mouth_roi).float()\n",
    "    mouth_roi_tensor = mouth_roi_tensor.permute(2, 0, 1)  # HWC → CHW\n",
    "    \n",
    "    return mouth_roi_tensor\n",
    "\n",
    "\n",
    "def extract_mouth_frames(\n",
    "    video_path: Path,\n",
    "    start_s: float,\n",
    "    end_s: float,\n",
    "    target_fps: int = 25,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Extract mouth ROI frames from video segment at fixed FPS.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        start_s: Start time in seconds\n",
    "        end_s: End time in seconds\n",
    "        target_fps: Frame sampling rate (25 fps standard)\n",
    "    \n",
    "    Returns:\n",
    "        Frame tensor [num_frames, 3, 112, 112]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Calculate frame indices\n",
    "        start_frame = int(start_s * fps)\n",
    "        end_frame = int(end_s * fps)\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        start_frame = max(0, min(start_frame, total_frames - 1))\n",
    "        end_frame = max(start_frame + 1, min(end_frame, total_frames))\n",
    "        \n",
    "        frames = []\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        for _ in range(start_frame, end_frame):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            mouth_roi = extract_mouth_roi_simple(frame)\n",
    "            frames.append(mouth_roi)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if not frames:\n",
    "            raise RuntimeError(f\"No frames extracted from window {start_s}-{end_s}s\")\n",
    "        \n",
    "        # Stack into tensor [num_frames, 3, H, W]\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        return frames_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Video frame extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def preprocess_video(video_frames: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize video frames using ImageNet statistics.\n",
    "    \n",
    "    Args:\n",
    "        video_frames: Raw frame tensor [num_frames, 3, H, W] in [0, 255]\n",
    "    \n",
    "    Returns:\n",
    "        Normalized frames on correct device\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    if video_frames.max() > 1.0:\n",
    "        video_frames = video_frames / 255.0\n",
    "    \n",
    "    # Apply ImageNet normalization\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=IMAGENET_MEAN,\n",
    "        std=IMAGENET_STD\n",
    "    )\n",
    "    \n",
    "    video_frames = normalize(video_frames)\n",
    "    return video_frames.to(DEVICE)\n",
    "\n",
    "\n",
    "print(\"✓ Video preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47357b",
   "metadata": {},
   "source": [
    "## Section 4: Video Preprocessing Pipeline\n",
    "\n",
    "Extract and preprocess mouth ROI frames:\n",
    "- Load video frames\n",
    "- Extract mouth region-of-interest (ROI)\n",
    "- Resize to 112×112 (model input size)\n",
    "- Normalize using ImageNet statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193132ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_segment(\n",
    "    video_path: Path,\n",
    "    start_s: float,\n",
    "    end_s: float,\n",
    "    target_sr: int = 16000,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Extract audio segment from video and return as normalized torch.Tensor.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        start_s: Start time in seconds\n",
    "        end_s: End time in seconds\n",
    "        target_sr: Target sample rate (16 kHz for Wav2Vec2)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape [1, num_samples], normalized to [-1, 1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load full audio from video\n",
    "        audio, sr = librosa.load(str(video_path), sr=None, mono=True)\n",
    "        \n",
    "        # Convert time to samples\n",
    "        start_sample = int(start_s * sr)\n",
    "        end_sample = int(end_s * sr)\n",
    "        \n",
    "        # Extract segment\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != target_sr:\n",
    "            audio_segment = librosa.resample(\n",
    "                audio_segment, orig_sr=sr, target_sr=target_sr\n",
    "            )\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        max_val = np.abs(audio_segment).max()\n",
    "        if max_val > 0:\n",
    "            audio_segment = audio_segment / (max_val + 1e-8)\n",
    "        \n",
    "        # Convert to tensor [1, T]\n",
    "        audio_tensor = torch.from_numpy(audio_segment).float()\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "        \n",
    "        return audio_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Audio extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def preprocess_audio(audio_waveform: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Final audio preprocessing before model input.\n",
    "    \n",
    "    Args:\n",
    "        audio_waveform: Raw audio tensor [1, T] or [T]\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed audio on correct device\n",
    "    \"\"\"\n",
    "    if audio_waveform.dim() == 1:\n",
    "        audio_waveform = audio_waveform.unsqueeze(0)\n",
    "    \n",
    "    # Ensure normalized\n",
    "    max_val = audio_waveform.abs().max()\n",
    "    if max_val > 0:\n",
    "        audio_waveform = audio_waveform / (max_val + 1e-8)\n",
    "    \n",
    "    return audio_waveform.to(DEVICE)\n",
    "\n",
    "\n",
    "print(\"✓ Audio preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8a8d2",
   "metadata": {},
   "source": [
    "## Section 3: Audio Preprocessing Pipeline\n",
    "\n",
    "Extract and preprocess audio segments from video files:\n",
    "- Load from video file\n",
    "- Resample to 16 kHz (Wav2Vec2 standard)\n",
    "- Convert to mono\n",
    "- Normalize to [-1, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eed533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalPhonemeVisemeAlignmentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-modal model for detecting lip-sync manipulation (deepfakes).\n",
    "    \n",
    "    Architecture:\n",
    "    - Audio encoder: Wav2Vec2 (precomputed or fine-tuned)\n",
    "    - Visual encoder: ResNet-18 CNN on mouth ROI frames\n",
    "    - Fusion: Cross-modal attention mechanism\n",
    "    - Classifier: FC head → logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dim: int = 768,  # Wav2Vec2 hidden size\n",
    "        visual_dim: int = 512,  # ResNet-18 feature dim\n",
    "        shared_dim: int = 256,  # Fusion embedding size\n",
    "        num_classes: int = 1,  # Binary: real vs fake\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio encoder (using pretrained Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.audio_encoder.freeze_feature_extractor()  # Freeze CNN feature extractor\n",
    "        self.audio_proj = nn.Linear(audio_dim, shared_dim)\n",
    "        \n",
    "        # Visual encoder (ResNet-18 backbone)\n",
    "        from torchvision.models import resnet18\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.visual_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.visual_proj = nn.Linear(512, shared_dim)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.attention_audio = nn.MultiheadAttention(\n",
    "            shared_dim, num_heads=4, batch_first=True\n",
    "        )\n",
    "        self.attention_visual = nn.MultiheadAttention(\n",
    "            shared_dim, num_heads=4, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Bottleneck (temporal pooling + fusion)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(shared_dim * 2, shared_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(shared_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        audio_waveform: Tensor,  # [1, num_samples]\n",
    "        video_frames: Tensor,  # [num_frames, 3, H, W]\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: extract features, fuse, classify.\n",
    "        \n",
    "        Returns:\n",
    "            logits: [1] or [batch_size, 1] raw scores (before sigmoid)\n",
    "        \"\"\"\n",
    "        # Audio processing\n",
    "        audio_features = self.audio_encoder(audio_waveform).last_hidden_state  # [1, T_audio, 768]\n",
    "        audio_features = self.audio_proj(audio_features)  # [1, T_audio, 256]\n",
    "        \n",
    "        # Visual processing\n",
    "        # Batch process frames through CNN\n",
    "        visual_features_list = []\n",
    "        for frame in video_frames:\n",
    "            feat = self.visual_encoder(frame.unsqueeze(0))  # [1, 512, 1, 1]\n",
    "            feat = feat.view(1, 512)  # [1, 512]\n",
    "            visual_features_list.append(feat)\n",
    "        visual_features = torch.cat(visual_features_list, dim=0)  # [num_frames, 512]\n",
    "        visual_features = self.visual_proj(visual_features)  # [num_frames, 256]\n",
    "        visual_features = visual_features.unsqueeze(0)  # [1, num_frames, 256]\n",
    "        \n",
    "        # Cross-modal attention (query audio, key/value visual)\n",
    "        audio_attended, _ = self.attention_audio(\n",
    "            audio_features, visual_features, visual_features\n",
    "        )  # [1, T_audio, 256]\n",
    "        \n",
    "        # Cross-modal attention (query visual, key/value audio)\n",
    "        visual_attended, _ = self.attention_visual(\n",
    "            visual_features, audio_features, audio_features\n",
    "        )  # [1, num_frames, 256]\n",
    "        \n",
    "        # Temporal pooling (mean over time)\n",
    "        audio_pooled = audio_attended.mean(dim=1)  # [1, 256]\n",
    "        visual_pooled = visual_attended.mean(dim=1)  # [1, 256]\n",
    "        \n",
    "        # Fusion bottleneck\n",
    "        fused = torch.cat([audio_pooled, visual_pooled], dim=-1)  # [1, 512]\n",
    "        bottleneck_out = self.bottleneck(fused)  # [1, 256]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(bottleneck_out)  # [1, 1]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"✓ Model architecture defined: CrossModalPhonemeVisemeAlignmentModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bc993",
   "metadata": {},
   "source": [
    "## Section 2: Define Model Architecture\n",
    "\n",
    "The **CrossModalPhonemeVisemeAlignmentModel** fuses audio and visual representations:\n",
    "\n",
    "1. **Audio Branch**: Wav2Vec2 encoder → audio embeddings (T_audio, D_audio)\n",
    "2. **Visual Branch**: ResNet-18 CNN → visual embeddings (T_video, D_visual)  \n",
    "3. **Fusion**: Linear projections to shared embedding space\n",
    "4. **Cross-Modal Attention**: Align audio-visual features\n",
    "5. **Bottleneck**: Shared representation (temporal pooling)\n",
    "6. **Classifier**: FC layer → raw logits (0/1 for real/fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fa74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "from torchvision import transforms\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✓ Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d5513",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
