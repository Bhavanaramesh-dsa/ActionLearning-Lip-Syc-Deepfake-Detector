##  Lip-Sync Deepfake Detector
## Audioâ€“Visual Temporal Inconsistency Modeling (Action Learning â€“ DSA12)

An explainable AI system for detecting lip-sync deepfakes by modeling cross-modal temporal alignment between speech and lip motion.

Unlike traditional deepfake detection systems that rely on visual artifacts, this project analyzes:

Phonemeâ€“viseme alignment

Audioâ€“visual synchronization stability

Window-level temporal inconsistency

Alignment variance & uncertainty

## ğŸ§  Problem Motivation

Modern lip-sync deepfakes:

Preserve speaker identity

Modify only the mouth region

Appear realistic frame-by-frame

Contain minimal visible artifacts

However:

Natural human speech contains strong temporal coupling between phonemes (audio) and visemes (lip shapes).

Deepfake systems often fail to maintain consistent alignment over time.

This system detects those inconsistencies.

## ğŸ— System Architecture
![System Architecture](LipSyncImages/SystemArchitecture.png)

## Pipeline Overview

Input Video (Audio + Video)

Preprocessing

Audio extraction

Mouth ROI cropping

Temporal windowing

Pretrained Encoders

Audio encoder (speech-aware)

Visual encoder (lip-motion focused)

Cross-Modal Alignment Module

Phonemeâ€“viseme alignment

Cross-attention

Shared bottleneck representation

Temporal Aggregation & Stability Analysis

Mean alignment score

Variance across windows

Classifier (MLP)

Final Decision & Risk Assessment

REAL / FAKE / UNCERTAIN

Confidence score

Risk level

## ğŸ” Detection Logic
Window-Based Temporal Analysis

Instead of classifying the entire video:

The video is divided into overlapping windows

Each window is independently scored

Scores are aggregated for final prediction

This enables:

Robust detection

Temporal localization

Reduced false positives

Stability estimation

Responsible AI â€“ UNCERTAIN Policy

## The system outputs:

REAL

FAKE

UNCERTAIN

UNCERTAIN is triggered when:

Scores are near the decision boundary

Windows strongly disagree

Evidence is ambiguous

This prevents over-confident false accusations.

## ğŸ–¥ Application Interface
## ğŸ” Login Page
![Login](LipSyncImages/Login.png)

Secure authentication for controlled local deployment.

## ğŸ¥ Single Video Analysis
![Single Prediction](LipSyncImages/SinglePrediction.png)

## Displays:

REAL / FAKE / UNCERTAIN

Confidence score

Risk level

Misaligned time segments

Deterministic explanation

PDF export

## ğŸ“Š Alignment Visualizations
![Heatmap](LipSyncImages/Heatmap.png)

Includes:

Timeline heatmap

Window misalignment score curve

Temporal stability visualization

These features make the system interpretable rather than a black box.

## ğŸ“ Batch Processing Dashboard
![Batch Dashboard](LipSyncImages/BatchPrediction.png)

Supports:

Multi-video upload

Per-video independent inference

Progress tracking

Structured results table

Batch-level review

## ğŸ’¬ Rule-Based Explanation Assistant
![Rulebased Chat](LipSyncImages/RulebasedChat.png)

Deterministic explanation system grounded in model outputs.

Important:

Not a generative AI chatbot

No hallucinations

Responses derived directly from detection metrics

## ğŸ“Š Output Metrics

For each analyzed video, the system provides:

Prediction (REAL / FAKE / UNCERTAIN)

Confidence score (0â€“1)

Risk level (Low / Medium / High)

Misalignment ratio (%)

Stability indicator

Highlighted suspicious segments

PDF forensic report

## ğŸ“‚ Project Structure

```text
ActionLearning-Lip-Syc-Deepfake-Detector/
â”‚
â”œâ”€â”€ app/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ analysis.py
â”‚   â”œâ”€â”€ model_runtime.py
â”‚   â”œâ”€â”€ windowing.py
â”‚   â”œâ”€â”€ quality_check.py
â”‚   â”œâ”€â”€ heatmap.py
â”‚   â”œâ”€â”€ report.py
â”‚   â”œâ”€â”€ chatbot.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ web/                      # Frontend UI
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ LipSyncImages/            # README assets
â”‚   â”œâ”€â”€ SystemArchitecture.png
â”‚   â”œâ”€â”€ Login.png
â”‚   â”œâ”€â”€ SinglePrediction.png
â”‚   â”œâ”€â”€ Heatmap.png
â”‚   â”œâ”€â”€ BatchPrediction.png
â”‚   â””â”€â”€ RulebasedChat.png
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_server.py
â””â”€â”€ README.md
```

## ğŸ›  Tech Stack
Backend

FastAPI

PyTorch

FFmpeg

NumPy

ReportLab

Frontend

HTML

CSS

JavaScript

Deployment

Docker

Hugging Face Spaces compatible

## ğŸš€ How to Run Locally
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
2ï¸âƒ£ Start the Server
python run_server.py
3ï¸âƒ£ Open in Browser
http://localhost:8000

Default login:

Username: admin
Password: admin123
## ğŸ“ Academic Context

Developed as part of:

EPITA â€“ DSA12 (Action Learning)
Topic: Detecting Lip-Sync Deepfakes via Audioâ€“Visual Temporal Inconsistency

Focus Areas:

Multimodal modeling

Temporal reasoning

Efficiency vs complexity trade-offs

Interpretability

Responsible AI

## ğŸ” Responsible AI Commitment

This system:

Explicitly models uncertainty

Separates explanation from prediction

Does not override predictions based on quality score

Provides temporal localization

Avoids black-box-only decisions

## ğŸŒ± Future Improvements

Fully trained large-scale audioâ€“visual model

Cross-dataset generalization evaluation

Real-time inference optimization

GPU deployment

Enhanced phonemeâ€“viseme interpretability maps

## ğŸ‘¤ Author

Bhavana Ramesh,
Ayush Chalise,
Vaishav Varma

Masterâ€™s in Data Science & Analytics
EPITA
